{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Intel Image Classification With Pytorch and Transfer Learning**\n\nIn this notebook, we will preprocess the intel images, we will create augemented images to increase the training dataset.\n\nThen we will use ResNet pre-trained model from torchvision and used the model to train in on our dataset\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Introducing adversarial attack to the original notebook\n\nThe first half of the code is from https://www.kaggle.com/asollie/intel-image-multiclass-pytorch-94-test-acc\n\nYou can refer to my github to see the full explaination of the code\nhttps://github.com/andylow1704/Adversarial_Attack"},{"metadata":{},"cell_type":"markdown","source":"# Load Packages"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\nimport PIL.Image as Image\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom matplotlib.ticker import MaxNLocator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom glob import glob\nimport shutil\nfrom collections import defaultdict\n\nimport torch, torchvision\nfrom torch import nn, optim #Torch NN(Conv, Pooling, etc.), Optimization.\nfrom torch.optim import lr_scheduler #Optimization LR Scheduler.\nimport torch.nn.functional as F #NN Functional.\nimport torchvision.transforms as T #Torchvision Transforms.\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torchvision import models #Torchvistion Models.\n\n%matplotlib inline\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\n\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n\nrcParams['figure.figsize'] = 15, 10\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Folders"},{"metadata":{},"cell_type":"markdown","source":"**We will get each label folder and we can see that we have 6 folders**\n\n* buildings = 0 \n* forest = 1\n* glacier = 2\n* mountain = 3\n* sea = 4\n* street = 5 "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_folders = sorted(glob('../input/intel-image-classification/seg_train/seg_train/*'))\nlen(train_folders)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load & View Images"},{"metadata":{},"cell_type":"markdown","source":"**Here we build 3 helpers to load and view images**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image(img_path, resize=True):\n    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n    \n    if resize:\n        img = cv2.resize(img, (64,64), interpolation = cv2.INTER_AREA)\n    \n    return img\n\ndef show_image(img_path):\n    img = load_image(img_path)\n    plt.imshow(img)\n    plt.axis('off')\n    \ndef show_sign_grid(image_paths):\n    images = [load_image(img) for img in image_paths]\n    images = torch.as_tensor(images)\n    images = images.permute(0, 3, 1, 2)\n    grid_img = torchvision.utils.make_grid(images, nrow = 11)\n    plt.figure(figsize = (24, 12))\n    plt.imshow(grid_img.permute(1, 2, 0))\n    plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sample of images for all classes we have**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#All Classes.\nsample_images = [np.random.choice(glob(f'{tf}/*jpg')) for tf in train_folders]\nshow_sign_grid(sample_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Buildings Class.\nimg_path = glob(f'{train_folders[0]}/*jpg')[1]\nshow_image(img_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train set images class distribution."},{"metadata":{},"cell_type":"markdown","source":"* buildings = 0 \n* forest = 1\n* glacier = 2\n* mountain = 3\n* sea = 4\n* street = 5 "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Class Classification.\nclass_names = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n\nclass_indices = [0, 1, 2, 3, 4, 5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will copy all the images to new dir, the purpose is to make it easier to torchvision dataset helpers to utilize the images**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train & Valid Directory.\n!rm -rf data\n\nDATA_DIR = Path('data')\n\nDATASETS = ['train', 'val']\n\nfor ds in DATASETS:\n    for cls in class_names:\n        (DATA_DIR / ds / cls).mkdir(parents=True, exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We are going to reserve 80% for train and 20% for validation for each class, then copy them to the correct folder**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Class Distribution & Split.\nfor i, cls_index in enumerate(class_indices):\n    image_paths = np.array(glob(f'{train_folders[cls_index]}/*jpg')) #train_folders[cls_index]를 image_paths에 저장.\n    class_name = class_names[i] #class_names[i]를 class_name에 저장.\n    print(f'{class_name}: {len(image_paths)}') #class_name과 len(image_paths)을 pirnt.\n    np.random.shuffle(image_paths)\n    \n    ds_split = np.split(\n        image_paths,\n        indices_or_sections = [int(.8 * len(image_paths)), int(.9 * len(image_paths))]\n    )\n    \n    dataset_data = zip(DATASETS, ds_split)\n    for ds, images in dataset_data:\n        for img_path in images:\n            shutil.copy(img_path, f'{DATA_DIR}/{ds}/{class_name}/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Requirement of ResNet"},{"metadata":{},"cell_type":"markdown","source":"**Distribution of classes are good, the total per class ratio is not so high**\n\n**We will apply some image augmentation techniques to artifically increase the size of dataset, we will apply some random resizing, rotation and horizontal flips, then we normalize the tensors using present values for each channel, this is requirement of ResNet**\n\n**Requirement of ResNet에 맞게 변경하는 작업.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforms.\nmean_nums = [0.485, 0.456, 0.406]\nstd_nums = [0.229, 0.224, 0.225]\n\ntransforms = {'train': T.Compose([\n    T.RandomResizedCrop(size = 256),\n    T.RandomRotation(degrees = 15),\n    T.RandomHorizontalFlip(),\n    T.ToTensor(),\n    T.Normalize(mean_nums, std_nums)]),\n    'val': T.Compose([\n    T.Resize(size = 256),\n    T.CenterCrop(size = 224),\n    T.ToTensor(),\n    T.Normalize(mean_nums, std_nums)]),\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will create pytorch dataset for image dataset and dataloaders**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Image Datasets.\nimage_datasets = {\n    d: ImageFolder(f'{DATA_DIR}/{d}', transforms[d]) for d in DATASETS\n}\n\n#Data Loaders.\ndata_loaders = {\n    d: DataLoader(image_datasets[d], batch_size=4, shuffle=True, num_workers=4) for d in DATASETS\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(data_loaders['train']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will store each class total images for later use**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset Sizes.\ndataset_sizes = {d: len(image_datasets[d]) for d in DATASETS}\nclass_names = image_datasets['train'].classes\n\ndataset_sizes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets have a look at some sample images with all the transformations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transformations Sample Images.\ndef imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([mean_nums])\n    std = np.array([std_nums])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.axis('off')\n    \ninputs, classes = next(iter(data_loaders['train']))\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Using pretrained model to classify the images\n\nWe will use the pre-trained ResNet to classify this images\n1. We will import the model (import all weights and arch except we will change the output layer as number of output class is different from ResNet dataset)\n2. Convert it into training mode\n3. Train the model on new data\n4. Evaluate\n5. Hopefully celebrate :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Model\ndef create_model(n_classes):\n    #resnet34, resnet152, wide_resnet101_2, resnext101_32x8d\n    model = models.resnext101_32x8d(pretrained = True) \n    \n    n_features = model.fc.in_features\n    model.fc = nn.Linear(n_features, n_classes)\n    \n    return model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = create_model(len(class_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print Model Layer.\nbase_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will create 3 helpers function to encapsualte train and eval func**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train() #Convert to train mode\n    losses = []\n    correct_predictions = 0\n    \n    for inputs, labels in data_loader:\n        inputs = inputs.to(device) #Push array to gpu\n        labels = labels.to(device)\n        \n        outputs = model(inputs) #get prob of output per class\n        \n        _, preds = torch.max(outputs, dim=1) # get max of pred\n        loss = loss_fn(outputs, labels) # get loss\n        \n        correct_predictions += torch.sum(preds==labels)\n        \n        losses.append(loss.item())\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    scheduler.step()\n    \n    return correct_predictions.double() / n_examples, np.mean(losses)\n\ndef eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval() #Evaluation mode\n    \n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            \n            _, preds = torch.max(outputs, dim=1)\n            \n            loss = loss_fn(outputs, labels)\n            \n            correct_predictions += torch.sum(preds==labels)\n            \n            losses.append(loss.item())\n    \n    return correct_predictions.double() / n_examples, np.mean(losses) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluation is simple, we don't even do gradient calculations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, data_loaders, dataset_sizes, device, n_epochs=5):\n    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n    loss_fn = nn.CrossEntropyLoss().to(device)\n    \n    history = defaultdict(list)\n    best_accuracy = 0\n    \n    for epoch in range(n_epochs):\n        print(f'Epoch {epoch + 1}/{n_epochs}')\n        print('-' * 10)\n        \n        train_acc, train_loss = train_epoch(model, data_loaders['train'], loss_fn, \n                                            optimizer, device, scheduler, dataset_sizes['train'])\n        \n        print(f'Train loss {train_loss} accuracy {train_acc}')\n        \n        val_acc, val_loss = eval_model(model, data_loaders['val'], loss_fn, device, dataset_sizes['val'])\n        \n        print(f'Val loss {val_loss} accuracy {val_acc}')\n        print()\n        \n        history['train_acc'].append(train_acc)\n        history['train_loss'].append(train_loss)\n        history['val_acc'].append(val_acc)\n        history['val_loss'].append(val_loss)\n        \n        if val_acc > best_accuracy:\n            torch.save(model.state_dict(), 'best_model_state.bin')\n            best_accuracy = val_acc\n            \n    print(f'Best val accuracy: {best_accuracy}')\n    \n    model.load_state_dict(torch.load('best_model_state.bin'))\n    \n    return model, history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nbase_model, history = train_model(base_model, data_loaders, dataset_sizes, device, n_epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize Training History.\ndef plot_training_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n    ax1.plot(history['train_loss'], label='train loss')\n    ax1.plot(history['val_loss'], label='validation loss')\n    \n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend()\n    ax1.set_ylabel('Loss')\n    ax1.set_xlabel('Epoch')\n    \n    ax2.plot(history['train_acc'], label='train accuracy')\n    ax2.plot(history['val_acc'], label='validation accuracy')\n    \n    ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax2.set_ylim([-0.05, 1.05])\n    ax2.legend()\n    ax2.set_ylabel('Accuracy')\n    ax2.set_xlabel('Epoch')\n    \n    fig.suptitle('Training History')\n    \nplot_training_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Get test data from dataset folder for evaluation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_folders = sorted(glob('../input/intel-image-classification/seg_test/seg_test/*'))\nlen(test_folders)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show Random Sample Image.\nsample_images = [np.random.choice(glob(f'{tf}/*jpg')) for tf in test_folders]\nshow_sign_grid(sample_images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here i repeat the same data preprocess step as like the train dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Class Classification.\nclass_names = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n\nclass_indices = [0, 1, 2, 3, 4, 5]\n\n#Test Directory.\nDATASETS = ['test']\n\nfor ds in DATASETS:\n    for cls in class_names:\n        (DATA_DIR / ds / cls).mkdir(parents=True, exist_ok=True)\n\n#Class Distribution & Split.\nfor i, cls_index in enumerate(class_indices):\n    image_paths = np.array(glob(f'{test_folders[cls_index]}/*jpg'))\n    class_name = class_names[i]\n    print(f'{class_name}: {len(image_paths)}')\n    np.random.shuffle(image_paths)\n    \n    ds_split = np.split(\n        image_paths,\n        indices_or_sections=[int(.8 * len(image_paths)), int(.9 * len(image_paths))]\n    )\n    \n    dataset_data = zip(DATASETS, ds_split)\n    for ds, images in dataset_data:\n        for img_path in images:\n            shutil.copy(img_path, f'{DATA_DIR}/{ds}/{class_name}/')\n\n#Transforms.  \nmean_nums = [0.485, 0.456, 0.406]\nstd_nums = [0.229, 0.224, 0.225]\n\ntransforms = {'test': T.Compose([\n    T.Resize(size = 256),\n    T.CenterCrop(size= 224),\n    T.ToTensor(),\n    T.Normalize(mean_nums, std_nums)]),\n}\n\n#Image Datasets.\nimage_datasets = {\n    d: ImageFolder(f'{DATA_DIR}/{d}', transforms[d]) for d in DATASETS\n}\n\n#Data Loaders.\ndata_loaders = {\n    d: DataLoader(image_datasets[d], batch_size=4, shuffle=True, num_workers=4) for d in DATASETS\n}\n\n#Dataset Sizes.\ndataset_sizes = {d: len(image_datasets[d]) for d in DATASETS}\nclass_names = image_datasets['test'].classes\n\ndataset_sizes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show Predictions.\ndef show_predictions(model, class_names, n_images=6):\n    model = model.eval()\n    images_handeled = 0\n    plt.figure()\n    \n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loaders['test']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            \n            _, preds = torch.max(outputs, 1)\n            \n            for j in range(inputs.shape[0]):\n                images_handeled += 1\n                ax = plt.subplot(2, n_images//2, images_handeled)\n                ax.set_title(f'predicted: {class_names[preds[j]]}')\n                imshow(inputs.cpu().data[j])\n                ax.axis('off')\n                \n                if images_handeled == n_images:\n                    return\n                \nshow_predictions(base_model, class_names, n_images=8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So based on the above figure, it seems like there is 0 wrong prediction,and 8 is correct, that is not bad, now lets see the classification report to understand the bigger view of model performance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification Report.\ndef get_predictions(model, data_loader):\n    model = model.eval()\n    predictions = []\n    real_values = []\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            \n            _, preds = torch.max(outputs, 1)\n            predictions.extend(preds)\n            real_values.extend(labels)\n    predictions = torch.as_tensor(predictions).cpu()\n    real_values = torch.as_tensor(real_values).cpu()\n    \n    return predictions, real_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing on unmodified images\n\ny_pred, y_test = get_predictions(base_model, data_loaders['test'])\n\nprint(classification_report(y_test, y_pred, target_names=class_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introducing Adversarial attack on test dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"# FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Full modified images \n\ndef get_predictions_adv(model, data_loader,epsilon):\n    \n    model = model.eval()\n    predictions = []\n    real_values = []\n    loss_fn = nn.CrossEntropyLoss().to(device) # Will not be using this \n    \n      \n    for inputs, labels in data_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # Set requires_grad attribute of tensor. Important for Attack\n        inputs.requires_grad = True\n            \n        # Forward pass the data through the model\n        outputs = model(inputs)\n        \n        \n        # Calculate the loss\n        loss = loss_fn(outputs, labels)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = inputs.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(inputs, epsilon, data_grad)\n        \n        # Re-classify the perturbed image\n        outputs = model(perturbed_data)\n        \n        ## New output  \n        \n        _, preds = torch.max(outputs, dim=1) # get max of pred\n        \n        predictions.extend(preds)\n        real_values.extend(labels)\n             \n            \n    predictions = torch.as_tensor(predictions).cpu()\n    real_values = torch.as_tensor(real_values).cpu()\n    \n    return predictions, real_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Testing model on full modified images \n\nfrom sklearn.metrics import accuracy_score\n\nepsilons = [0, .05, .1, .15, .2, .25, .3]\naccuracy = [] \n\n# Run test for each epsilon\nfor eps in epsilons:\n    print(\"For Epsilon =\" +str(eps))\n    y_pred, y_test = get_predictions_adv(base_model, data_loaders['test'],eps)\n    \n    acc = accuracy_score(y_pred,y_test)\n    accuracy.append(acc)\n    \n    print(classification_report(y_test, y_pred, target_names=class_names))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot epsilon accuracy graph\n\nplt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracy, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing on mixture of modified and unmodified images"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Test sets with combination of modified images and unmodified images \nimport random\n\ndef get_predictions_random(model, data_loader):\n    \n    model = model.eval()\n    predictions = []\n    real_values = []\n    \n    loss_fn = nn.CrossEntropyLoss().to(device)\n      \n    for inputs, labels in data_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n    \n        # Get probability \n        prob = random.uniform(0,1)\n        \n        if prob <0.5:\n            \n            ##Generate random epsilon\n            epsilon = random.uniform(0,0.3)\n            \n            # Set requires_grad attribute of tensor. Important for Attack\n            inputs.requires_grad = True\n\n            # Forward pass the data through the model\n            outputs = model(inputs)\n\n\n            # Calculate the loss\n            loss = loss_fn(outputs, labels)\n\n            # Zero all existing gradients\n            model.zero_grad()\n\n            # Calculate gradients of model in backward pass\n            loss.backward()\n\n            # Collect datagrad\n            data_grad = inputs.grad.data\n\n            # Call FGSM Attack\n            perturbed_data = fgsm_attack(inputs, epsilon, data_grad)\n\n            # Re-classify the perturbed image\n            outputs = model(perturbed_data)\n\n            ## New output  \n            _, preds = torch.max(outputs, 1)\n        \n        \n            predictions.extend(preds)\n            real_values.extend(labels)\n            \n        else: \n            outputs = model(inputs)\n            \n            _, preds = torch.max(outputs, 1)\n            predictions.extend(preds)\n            real_values.extend(labels)\n            \n            \n    predictions = torch.as_tensor(predictions).cpu()\n    real_values = torch.as_tensor(real_values).cpu()\n    \n    return predictions, real_values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Report using combination of modified and unmodified images\n\ny_pred, y_test = get_predictions_random(base_model, data_loaders['test'])\nprint(classification_report(y_test, y_pred, target_names=class_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing on full randomly modified images"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Test sets with full modified images\n\ndef get_predictions_adv2(model, data_loader):\n    \n    model = model.eval()\n    predictions = []\n    real_values = []\n    \n    loss_fn = nn.CrossEntropyLoss().to(device)\n      \n    for inputs, labels in data_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n    \n    \n            \n        ##Generate random epsilon\n        epsilon = random.uniform(0,0.3)\n            \n        # Set requires_grad attribute of tensor. Important for Attack\n        inputs.requires_grad = True\n\n        # Forward pass the data through the model\n        outputs = model(inputs)\n\n\n        # Calculate the loss\n        loss = loss_fn(outputs, labels)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = inputs.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(inputs, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        outputs = model(perturbed_data)\n\n        ## New output  \n        _, preds = torch.max(outputs, 1)\n        \n        \n        predictions.extend(preds)\n        real_values.extend(labels)\n            \n            \n            \n            \n    predictions = torch.as_tensor(predictions).cpu()\n    real_values = torch.as_tensor(real_values).cpu()\n    \n    return predictions, real_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Report for the full modified images \n\ny_pred, y_test = get_predictions_adv2(base_model, data_loaders['test'])\nprint(classification_report(y_test, y_pred, target_names=class_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training a model with mixture of modified and unmodified images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train() #Convert to train mode\n    losses = []\n    correct_predictions = 0\n    \n    for inputs, labels in data_loader:\n        \n        prob = random.uniform(0,1) ## generating probability\n        \n        inputs = inputs.to(device) #Push array to gpu\n        labels = labels.to(device)\n        \n        if prob < 0.5:\n            \n            epsilon = random.uniform(0,0.3) ## generating random epsilon \n        \n            # Set requires_grad attribute of tensor. Important for Attack\n            inputs.requires_grad = True\n\n            # Forward pass the data through the model\n            outputs = model(inputs) \n\n\n            # Calculate the loss\n            loss = loss_fn(outputs, labels)\n\n            # Zero all existing gradients\n            model.zero_grad()\n\n            # Calculate gradients of model in backward pass\n            loss.backward()\n\n            # Collect datagrad\n            data_grad = inputs.grad.data\n\n            # Call FGSM Attack\n            perturbed_data = fgsm_attack(inputs, epsilon, data_grad)\n            \n            # Re-classify the perturbed image\n            outputs = model(perturbed_data)\n           \n            \n            _, preds = torch.max(outputs, dim=1) # get max of pred\n            correct_predictions += torch.sum(preds==labels)\n                \n            \n            loss = loss_fn(outputs, labels) # get loss\n            losses.append(loss.item())\n        \n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n        else : \n\n            outputs = model(inputs) #get prob of output per class\n\n\n            loss = loss_fn(outputs, labels) # get loss\n            losses.append(loss.item())\n            \n            _, preds = torch.max(outputs, dim=1) # get max of pred\n            correct_predictions += torch.sum(preds==labels)\n            \n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    scheduler.step()\n    \n    return correct_predictions.double() / n_examples, np.nanmean(losses)\n\n\n\ndef eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval() #Evaluation mode\n    losses = []\n    correct_predictions = 0\n    \n    \n    for inputs, labels in data_loader:\n        \n        prob = random.uniform(0,1) ## generating probability\n        \n        inputs = inputs.to(device)\n        labels = labels.to(device)\n    \n        \n        if prob <0.5:\n            \n            epsilon = random.uniform(0,0.3) ## generating random epsilon \n        \n            # Set requires_grad attribute of tensor. Important for Attack\n            inputs.requires_grad = True\n\n            # Forward pass the data through the model\n            outputs = model(inputs)\n            \n        \n            # Calculate the loss\n            loss = loss_fn(outputs, labels)\n\n            # Zero all existing gradients\n            model.zero_grad()\n\n            # Calculate gradients of model in backward pass\n            loss.backward()\n\n            # Collect datagrad\n            data_grad = inputs.grad.data\n\n            # Call FGSM Attack\n            perturbed_data = fgsm_attack(inputs, epsilon, data_grad)\n            \n            # Re-classify the perturbed image\n            outputs = model(perturbed_data)\n\n            _, preds = torch.max(outputs, dim=1) \n\n            correct_predictions += torch.sum(preds==labels)\n            \n            loss = loss_fn(outputs,labels)\n            losses.append(loss.item())\n        \n        else:\n        \n            outputs = model(inputs)\n      \n            loss = loss_fn(outputs,labels)\n            losses.append(loss.item())\n            \n            _, preds = torch.max(outputs, dim=1)\n            \n            correct_predictions += torch.sum(preds==labels)\n          \n        \n    return correct_predictions.double() / n_examples, np.nanmean(losses) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, data_loaders, dataset_sizes, device, n_epochs=5):\n    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n    loss_fn = nn.CrossEntropyLoss().to(device) \n    history = defaultdict(list)\n    best_accuracy = 0\n    \n    for epoch in range(n_epochs):\n        print(f'Epoch {epoch + 1}/{n_epochs}')\n        print('-' * 10)\n        \n        train_acc, train_loss = train_epoch(model, data_loaders['train'], loss_fn, \n                                            optimizer, device, scheduler, dataset_sizes['train'])\n        \n        print(f'Train loss {train_loss} accuracy {train_acc}')\n        \n        val_acc, val_loss = eval_model(model, data_loaders['val'], loss_fn, device, dataset_sizes['val'])\n        \n        print(f'Val loss {val_loss} accuracy {val_acc}')\n        print()\n        \n        history['train_acc'].append(train_acc)\n        history['train_loss'].append(train_loss)\n        history['val_acc'].append(val_acc)\n        history['val_loss'].append(val_loss)\n        \n        if val_acc > best_accuracy:\n            torch.save(model.state_dict(), 'best_model_state.bin')\n            best_accuracy = val_acc\n            \n    print(f'Best val accuracy: {best_accuracy}')\n    \n    model.load_state_dict(torch.load('best_model_state.bin'))\n    \n    return model, history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nbase_model, history = train_model(base_model, data_loaders, dataset_sizes, device, n_epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize Training History.\ndef plot_training_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n    ax1.plot(history['train_loss'], label='train loss')\n    ax1.plot(history['val_loss'], label='validation loss')\n    \n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend()\n    ax1.set_ylabel('Loss')\n    ax1.set_xlabel('Epoch')\n    \n    ax2.plot(history['train_acc'], label='train accuracy')\n    ax2.plot(history['val_acc'], label='validation accuracy')\n    \n    ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax2.set_ylim([-0.05, 1.05])\n    ax2.legend()\n    ax2.set_ylabel('Accuracy')\n    ax2.set_xlabel('Epoch')\n    \n    fig.suptitle('Training History')\n    \nplot_training_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing model using a combination of unmodified and modified images"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Result(Mixture of modified and unmodified images)\n\ny_pred, y_test = get_predictions_random(base_model, data_loaders['test'])\nprint(classification_report(y_test, y_pred, target_names=class_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing model using full set of unmodified images"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Result(Full randomly modified images)\n\ny_pred, y_test = get_predictions_adv2(base_model, data_loaders['test'])\nprint(classification_report(y_test, y_pred, target_names=class_names))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}